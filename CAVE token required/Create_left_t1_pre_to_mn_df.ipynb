{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "import connectome_create\n",
    "# viz_method = one of ['itkwidgets', 'vtk']\n",
    "viz_method = 'vtk'\n",
    "\n",
    "# import some of our favorite packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import nglui.statebuilder as ngstbld\n",
    "\n",
    "# this is the EM specific package for querying the EM data\n",
    "from caveclient import CAVEclient\n",
    "\n",
    "\n",
    "from meshparty import trimesh_io, trimesh_vtk\n",
    "from meshparty import skeletonize, skeleton_io, skeleton\n",
    "import cloudvolume\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# client = CAVEclient()\n",
    "\n",
    "# # if not os.path.isfile(os.path.expanduser(\"~/.cloudvolume/secrets/cave-secret.json\")):\n",
    "# client.auth.get_new_token(open=True)\n",
    "# # # if you have not yet setup this computer, uncomment this below line\n",
    "# # # paste the token from the website in, and run the line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.auth.save_token(token=\"fca0294ed408d104e0eebe74514c744b\", overwrite=True)\n",
    "\n",
    "# # # then comment or delete the line as you don't need to run it on this computer  again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datastack_name = 'fanc_production_mar2021'\n",
    "\n",
    "client = CAVEclient(datastack_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.materialize.get_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.materialize.get_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.materialize.get_table_metadata('nerve_bundle_fibers_v0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the pre_to_mn_df \n",
    "with index (segID,has_soma,sensory,neck,local) \n",
    "x \n",
    "(side, nerve,segment,function,muscle,rank,segID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterates down the multiindex of motor neurons, selecting similar motor neuron segID to query for presynaptic partners.\n",
    "# Inputs: client, mn_index and optional side\n",
    "# returns: \n",
    "\n",
    "t1_mns_df = client.materialize.query_table('motor_neuron_table_v7',timestamp=connectome_create.get_timestamp())\n",
    "t1_mns_df.index = connectome_create.mn_multi_index(t1_mns_df)\n",
    "All = slice(None)\n",
    "mn_index = t1_mns_df.loc[('L',All,All,All,All,All,All),:]\n",
    "full_left_df = client.materialize.synapse_query(post_ids = t1_mns_df.loc['L',:].pt_root_id.to_list(),timestamp=connectome_create.get_timestamp())\n",
    "print(full_left_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mns_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mns_df.index.to_frame().reset_index(drop=True).to_csv('./dfs_saved/mns_to_make_public_20230524.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_left_df.loc[:,['pre_pt_root_id','post_pt_root_id']]\n",
    "\n",
    "# Then use factorize, which encodes each occurance in a list, in this case of paired root_ids, pre and post\n",
    "ij,tups = pd.factorize(list(zip(*map(df.get,df))))\n",
    "\n",
    "# Then create a dictionary out of each occurance, storing the number of factorized codes\n",
    "result = dict(zip(tups, np.bincount(ij)))\n",
    "\n",
    "# Finally, turn the result into series, with the tuples of pt_root_ids as the multiindex\n",
    "tupseries = pd.Series(result)\n",
    "\n",
    "thresh = 3\n",
    "# Finally, threshold by the connection strength and perfom the unstack operation\n",
    "thresholdedtupseries = tupseries[tupseries>=thresh]\n",
    "prelim_pre_df = thresholdedtupseries.unstack(fill_value=0)\n",
    "\n",
    "# takes ~ 15 seconds,\n",
    "print(prelim_pre_df.shape)\n",
    "pre_to_mn_df = prelim_pre_df.loc[:,mn_index.index.get_level_values('segID').to_list()]\n",
    "pre_to_mn_df.columns = mn_index.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soma_table = client.materialize.query_table('neuron_somas_dec2022',timestamp=connectome_create.get_timestamp())\n",
    "neckns_df = client.materialize.query_table('neck_connective',timestamp=connectome_create.get_timestamp())\n",
    "all_sensory = connectome_create.get_unduplicated_sensory_axon_table(client)\n",
    "\n",
    "pmn_index = pre_to_mn_df.index.to_frame()\n",
    "pmn_index.rename(columns={0:'segID'},inplace=True)\n",
    "pmn_index['motor'] = pmn_index.segID.isin(mn_index.pt_root_id)\n",
    "pmn_index['has_soma'] = pmn_index.segID.isin(soma_table.pt_root_id)\n",
    "pmn_index['sensory'] = pmn_index.segID.isin(all_sensory.pt_root_id)\n",
    "pmn_index['neck'] = pmn_index.segID.isin(neckns_df.pt_root_id)\n",
    "pmn_index['local'] = False\n",
    "pmn_index.neck.sum(axis=0)\n",
    "pre_to_mn_df.index = pd.MultiIndex.from_frame(pmn_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pre_to_mn_df.sum(axis=0)\n",
    "pre_to_mn_df = pre_to_mn_df[s.sort_values(ascending=True).index]\n",
    "\n",
    "new_idx = utils.sort_segment_fcn_index(pre_to_mn_df.columns)\n",
    "pre_to_mn_df = pre_to_mn_df[new_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create cell_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Motor neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "m_r_tup = (All,True,All,All,All,All)\n",
    "mns = pre_to_mn_df.loc[m_r_tup,:]\n",
    "mns = mns.loc[mns.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print('mns shape: {}'.format(mns.shape))\n",
    "\n",
    "### sensory neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "s_r_tup = (All,False,False,True,False,False)\n",
    "sensory = pre_to_mn_df.loc[s_r_tup,:]\n",
    "sensory = sensory.loc[sensory.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print('sensory shape: {}'.format(sensory.shape))\n",
    "\n",
    "### descending neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "descending = pre_to_mn_df.loc[(All,False,False,False,True,All),:]\n",
    "descending = descending.loc[descending.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print('descending shape: {}'.format(descending.shape))\n",
    "\n",
    "### How many ascending neurons # 138\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "ascending = pre_to_mn_df.loc[(All,False,True,False,True,False),:]\n",
    "ascending = ascending.loc[ascending.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print('ascending shape: {}'.format(ascending.shape))\n",
    "\n",
    "### VNC neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "vncns = pre_to_mn_df.loc[(All,False,True,False,False,False),:]\n",
    "vncns = vncns.loc[vncns.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print('vncns shape: {}'.format(vncns.shape))\n",
    "\n",
    "## strange motor neurons\n",
    "sensmns = pre_to_mn_df.loc[(All,False,True,True,False,False),:]\n",
    "sensmns = sensmns.loc[sensmns.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print('sensmns shape: {}'.format(sensmns.shape))\n",
    "\n",
    "(mns.shape[0]+sensory.shape[0]+descending.shape[0] + ascending.shape[0] + vncns.shape[0] + sensmns.shape[0] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at fragments briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurons with a soma and not ascending\n",
    "fragments = pre_to_mn_df.loc[(All,False,False,False,False,False),:].copy()\n",
    "### VNC neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "fragments = pre_to_mn_df.loc[(All,False,False,False,False,False),:]\n",
    "fragments = fragments.loc[fragments.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "\n",
    "total = fragments.sum(axis=1)\n",
    "tot = total.cumsum()#/total.sum() # The fraction of total input from a given fragment\n",
    "n = np.array([i for i in range(len(tot))])  # assign a number to neurons\n",
    "frc = n/n.sum()\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "# ax1.scatter(x, y)\n",
    "ax1.plot(n,tot)#, marker='o')\n",
    "plt.sca(ax1)\n",
    "plt.title('Total synapses from N fragments')\n",
    "plt.ylabel('M synapses')\n",
    "plt.xlabel('N fragments')\n",
    "plt.show()\n",
    "# locs, labels = plt.xticks(ticks=x, labels=lbls, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(pre_to_mn_df,ext='edited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_pMN_to_MN_df = pd.concat([\n",
    "    mns,\n",
    "    descending,\n",
    "    sensory,\n",
    "    vncns,\n",
    "    ascending,\n",
    "    sensmns,\n",
    "    fragments\n",
    "])\n",
    "print(ordered_pMN_to_MN_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(ordered_pMN_to_MN_df,ext='ordered')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Premotor connectivity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_to_mn_df = ordered_pMN_to_MN_df # connectome_create.load_pre_to_mn_df(ext='ordered')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_to_mn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = slice(None)\n",
    "\n",
    "# # How many motor neurons are presynaptic to motor neurons\n",
    "# # segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "motor = pre_to_mn_df.loc[(All,True,True,All,False,False),:]\n",
    "# descending.index.get_level_values('segID').to_list()\n",
    "print(motor.shape)\n",
    "\n",
    "# How many sensory neurons # 94\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "sensory = pre_to_mn_df.loc[(All,False,False,True,False,False),:]\n",
    "# descending.index.get_level_values('segID').to_list()\n",
    "print(sensory.shape)\n",
    "\n",
    "# How many descending neurons # 217 214?\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "descending = pre_to_mn_df.loc[(All,False,False,False,True),:]\n",
    "# descending.index.get_level_values('segID').to_list()\n",
    "print(descending.shape)\n",
    "\n",
    "# How many motor neurons are presynaptic to motor neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "ascending = pre_to_mn_df.loc[(All,False,True,False,True,False),:]\n",
    "# descending.index.get_level_values('segID').to_list()\n",
    "print(ascending.shape)\n",
    "# some ascending that should be ascending\n",
    "# 648518346507084872\n",
    "# 648518346480786912\n",
    "# 648518346490374748\n",
    "sid = 648518346504729203\n",
    "ascending.index.get_level_values('segID')==sid\n",
    "# All are here\n",
    "\n",
    "# Fragments?\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "sensmns = pre_to_mn_df.loc[(All,False,True,True,False,False),:]\n",
    "print(sensmns.shape)\n",
    "\n",
    "# Fragments?\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "fragments = pre_to_mn_df.loc[(All,False,False,False,False,False),:]\n",
    "print(fragments.shape)\n",
    "\n",
    "# Local and intersegmental? # 880\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "vncns = pre_to_mn_df.loc[(All,False,True,False,False,False),:]\n",
    "# descending.index.get_level_values('segID').to_list()\n",
    "print(vncns.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find intersegmental neurons\n",
    "Go through the vnc premotor neurons and look for a synapse outside the left hand box.\n",
    "Once you find one, move to the next premotor neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soma box\n",
    "soma_table = client.materialize.query_table('somas_dec2022',timestamp=connectome_create.get_timestamp())\n",
    "soma_table.index = soma_table.pt_root_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx = [-3000, 3000]\n",
    "sy = [-3000, 3000]\n",
    "sz = [-300, 300]\n",
    "\n",
    "# T1 bounding box, click on neuroglancer instance\n",
    "All = slice(None)\n",
    "x = [3000 , 38000]\n",
    "y = [90483, 123190] # extended the boundaries a bit 5/10/22\n",
    "z = [980, 3858]\n",
    "\n",
    "t1 = lambda b: (b[0]>=x[0]) & (b[0]<=x[1]) & (b[1]>=y[0]) & (b[1]<=y[1]) & (b[2]>=z[0]) & (b[2]<=z[1])\n",
    "\n",
    "def get_non_T1_inputs(iln_syn_df, s):\n",
    "    in_T1_box = iln_syn_df.loc[:,['post_pt_position']].applymap(t1)\n",
    "    pts_out_of_T1 = iln_syn_df.loc[~in_T1_box.post_pt_position,:]\n",
    "    pts_in_T1 = iln_syn_df.loc[in_T1_box.post_pt_position,:]\n",
    "    nearsoma = lambda b: (b[0]>=s[0]+sx[0]) & (b[0]<=s[0]+sx[1]) & (b[1]>=s[1]+sy[0]) & (b[1]<=s[1]+sy[1]) & (b[2]>=s[2]+sz[0]) & (b[2]<=s[2]+sz[1])\n",
    "    in_soma_box = pts_out_of_T1.loc[:,['post_pt_position']].applymap(nearsoma)\n",
    "    pts_out_of_T1 = pts_out_of_T1.loc[~in_soma_box.post_pt_position,:]\n",
    "    return pts_out_of_T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "vncnsindex = vncns.index.to_frame()\n",
    "vncnsindex.index = vncnsindex.index.get_level_values('segID')\n",
    "vncnsindex['non_T1_count'] = 0\n",
    "\n",
    "row_i = 0\n",
    "row_f = 99\n",
    "cnt = 0\n",
    "while row_i<vncnsindex.shape[0]:\n",
    "    print('Querying [{}:{}]'.format(row_i,row_f))\n",
    "    rows_df = vncnsindex.iloc[row_i:row_f,:]\n",
    "    iln_syn_df = client.materialize.synapse_query(post_ids = rows_df.index.get_level_values('segID').to_list(),timestamp=connectome_create.get_timestamp())\n",
    "    for idx in rows_df.index.get_level_values('segID').to_list():\n",
    "        try:\n",
    "            s=soma_table.loc[idx,:].pt_position\n",
    "        except KeyError:\n",
    "            if 'fraglist' not in locals():\n",
    "                today = date.today()\n",
    "                d1 = today.strftime(\"%Y%m%d\")\n",
    "                fn = './dfs_saved/' + 'frag_df' + '_' + d1 + '.pkl'\n",
    "                fraglist= pd.read_pickle(fn)\n",
    "            if sid in fraglist.pt_root_id.to_list():\n",
    "                print('{} is a known fragment of a local neuron'.format(sid))\n",
    "                vncnsindex.loc[idx,'local'] = True\n",
    "        \n",
    "        non_T1_inputs = get_non_T1_inputs(iln_syn_df.loc[iln_syn_df['post_pt_root_id']==idx], s)\n",
    "        if non_T1_inputs.empty:\n",
    "            #print('{} is local'.format(sid))\n",
    "            vncnsindex.loc[idx,'local'] = True\n",
    "        else: \n",
    "            #print('{} has {} inputs outside of T1'.format(sid,non_T1_inputs.shape[0]))\n",
    "            vncnsindex.loc[idx,'non_T1_count'] = non_T1_inputs.shape[0]\n",
    "        \n",
    "    print('Done [{}:{}]'.format(row_i,row_f))\n",
    "    row_i = row_f+1\n",
    "    row_f = min([row_i+99,vncnsindex.shape[0]])\n",
    "    \n",
    "vncns.index = pd.MultiIndex.from_frame(vncnsindex)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconcatenate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnindex= motor.index.to_frame()\n",
    "mnindex.index = mnindex.index.get_level_values('segID')\n",
    "mnindex['non_T1_count'] = 0\n",
    "motor.index = pd.MultiIndex.from_frame(mnindex)\n",
    "\n",
    "descindex= descending.index.to_frame()\n",
    "descindex.index = descindex.index.get_level_values('segID')\n",
    "descindex['non_T1_count'] = 0\n",
    "descending.index = pd.MultiIndex.from_frame(descindex)\n",
    "\n",
    "sensoryindex= sensory.index.to_frame()\n",
    "sensoryindex.index = sensoryindex.index.get_level_values('segID')\n",
    "sensoryindex['non_T1_count'] = 0\n",
    "sensory.index = pd.MultiIndex.from_frame(sensoryindex)\n",
    "\n",
    "ascindex= ascending.index.to_frame()\n",
    "ascindex.index = ascindex.index.get_level_values('segID')\n",
    "ascindex['non_T1_count'] = 0\n",
    "ascending.index = pd.MultiIndex.from_frame(ascindex)\n",
    "\n",
    "sensmnsindex= sensmns.index.to_frame()\n",
    "sensmnsindex.index = sensmnsindex.index.get_level_values('segID')\n",
    "sensmnsindex['non_T1_count'] = 0\n",
    "sensmns.index = pd.MultiIndex.from_frame(sensmnsindex)\n",
    "\n",
    "frgindex= fragments.index.to_frame()\n",
    "frgindex.index = frgindex.index.get_level_values('segID')\n",
    "frgindex['non_T1_count'] = 0\n",
    "fragments.index = pd.MultiIndex.from_frame(frgindex)\n",
    "\n",
    "ordered_pMN_to_MN_df = pd.concat([\n",
    "    motor,\n",
    "    descending,\n",
    "    sensory,\n",
    "    vncns,\n",
    "    ascending,\n",
    "    sensmns,\n",
    "    fragments,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(ordered_pMN_to_MN_df,ext='vncns_inter_vs_local')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "pre_to_mn_df = ordered_pMN_to_MN_df\n",
    "\n",
    "m_r_tup = (All,True,All,All,All,All)\n",
    "motor = pre_to_mn_df.loc[m_r_tup,:]\n",
    "motor = motor.loc[motor.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print(motor.shape)\n",
    "\n",
    "### sensory neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "s_r_tup = (All,False,False,True,False,False)\n",
    "sensory = pre_to_mn_df.loc[s_r_tup,:]\n",
    "sensory = sensory.loc[sensory.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print(sensory.shape)\n",
    "\n",
    "### weird mn neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "w_r_tup = (All,False,True,True,False,False)\n",
    "sensmns = pre_to_mn_df.loc[w_r_tup,:]\n",
    "sensmns = sensmns.loc[sensmns.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print(sensmns.shape)\n",
    "\n",
    "### descending neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "descending = pre_to_mn_df.loc[(All,False,False,False,True,All),:]\n",
    "descending = descending.loc[descending.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print(descending.shape)\n",
    "\n",
    "### How many ascending neurons # 138\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "ascending = pre_to_mn_df.loc[(All,False,True,False,True,False),:]\n",
    "ascending = ascending.loc[ascending.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print(ascending.shape)\n",
    "\n",
    "### VNC neurons\n",
    "# segID\tmotor\thas_soma\tsensory\t neck\tlocal\t\n",
    "vncns = pre_to_mn_df.loc[(All,False,True,False,False,All,All),:]\n",
    "vncns = vncns.loc[vncns.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "print(vncns.shape)\n",
    "\n",
    "fragments = pre_to_mn_df.loc[(All,False,False,False,False,False),:]\n",
    "fragments.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reclassify\n",
    "Should some of these be local neurons instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local = vncns.loc[(All,False,True,False,False,True,All),:]\n",
    "print('local shape:{}'.format(local.shape))\n",
    "\n",
    "# peel off some local neurons from the intersegmental neurons\n",
    "intrseg = vncns.loc[(All,False,True,False,False,False,All),:]\n",
    "print('inter shape:{}'.format(intrseg.shape))\n",
    "\n",
    "index = intrseg.index.to_frame()\n",
    "index.index = index.index.get_level_values('segID')\n",
    "somenonT1 = index.non_T1_count<20\n",
    "intrseg.loc[somenonT1.to_list(),:]\n",
    "\n",
    "possible_local = intrseg.loc[somenonT1.to_list(),:]\n",
    "print(possible_local.shape)\n",
    "\n",
    "# Order local_p by numbers of synapses\n",
    "local_p = pd.concat([local,possible_local])\n",
    "local_p = local_p.loc[local_p.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "\n",
    "intrseg_m = intrseg.loc[~somenonT1.to_numpy(),:]\n",
    "\n",
    "print(intrseg_m.shape)\n",
    "\n",
    "# Print the next tranche of neurons to see if any others should be typed and local\n",
    "mi,arr = intrseg_m.index.sortlevel(level='non_T1_count')\n",
    "mi_df = mi.to_frame()\n",
    "\n",
    "# Now just look for that giant 19B neuron and move it to the local stack\n",
    "pmns = pd.DataFrame()\n",
    "pmns['pt_position'] = [[52132, 120825, 1557]]\n",
    "\n",
    "# looking to add this one particular neuron:\n",
    "cv = cloudvolume.CloudVolume(client.info.get_datastack_info()['segmentation_source'], use_https=True)\n",
    "segID = utils.segIDs_from_pts_service(pmns.pt_position,cv)\n",
    "\n",
    "if any(pre_to_mn_df.index.get_level_values('segID').isin([segID[0]])):\n",
    "    print('Moving large neuron {} from intersegmental to local group'.format(segID))\n",
    "    local_p = pd.concat([local_p,intrseg_m.loc[segID]])\n",
    "    local_p = local_p.loc[local_p.sum(axis=1).sort_values(ascending=False).index,:]\n",
    "\n",
    "\n",
    "locidx = local_p.index.to_frame()\n",
    "# locidx.index = locidx.index.get_level_values('segID').to_list()\n",
    "locidx.local = True\n",
    "locmi = pd.MultiIndex.from_frame(locidx)\n",
    "local_p.index = locmi\n",
    "print(local_p.shape)\n",
    "\n",
    "intrseg_m = intrseg_m.loc[mi_df.segID!=segID[0],:]\n",
    "print('local shape:{} ({})'.format(local_p.shape,locidx.local.sum()))\n",
    "print('inter shape:{}'.format(intrseg_m.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_p.head()\n",
    "segID[0]\n",
    "local_p.loc[648518346480658945]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort cell classes by similarity onto mns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descending neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(descending.shape)\n",
    "sim_mat = cosine_similarity(descending.to_numpy())\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(sim_mat)\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "clustered_order = utils.plot_dendrogram(model, truncate_mode=\"level\", p=6000) # p truncate mode\n",
    "clustered_order = np.array(clustered_order).astype(int) # convert strins into integers\n",
    "# print(clustered_order)\n",
    "descending_clustered = descending.iloc[clustered_order,:].copy()\n",
    "descending_clustered.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sensory neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(sensory.to_numpy())\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(sim_mat)\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "clustered_order = utils.plot_dendrogram(model, truncate_mode=\"level\", p=6000) # p truncate mode\n",
    "clustered_order = np.array(clustered_order).astype(int) # convert strins into integers\n",
    "# print(clustered_order)\n",
    "sensory_clustered = sensory.iloc[clustered_order,:].copy()\n",
    "sensory_clustered.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(local_p.to_numpy())\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(sim_mat)\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "clustered_order = utils.plot_dendrogram(model, truncate_mode=\"level\", p=6000) # p truncate mode\n",
    "clustered_order = np.array(clustered_order).astype(int) # convert strins into integers\n",
    "# print(clustered_order)\n",
    "local_clustered = local_p.iloc[clustered_order,:].copy()\n",
    "local_clustered.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersegmental neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(intrseg_m.to_numpy())\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(sim_mat)\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "clustered_order = utils.plot_dendrogram(model, truncate_mode=\"level\", p=6000) # p truncate mode\n",
    "clustered_order = np.array(clustered_order).astype(int) # convert strins into integers\n",
    "# print(clustered_order)\n",
    "intrseg_clustered = intrseg_m.iloc[clustered_order,:].copy()\n",
    "intrseg_clustered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = cosine_similarity(ascending.to_numpy())\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(sim_mat)\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "clustered_order = utils.plot_dendrogram(model, truncate_mode=\"level\", p=6000) # p truncate mode\n",
    "clustered_order = np.array(clustered_order).astype(int) # convert strins into integers\n",
    "# print(clustered_order)\n",
    "ascending_clustered = ascending.iloc[clustered_order,:].copy()\n",
    "ascending_clustered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_pMN_to_MN_df = pd.concat([\n",
    "    motor,\n",
    "    descending_clustered,\n",
    "    sensory_clustered,\n",
    "    intrseg_clustered,\n",
    "    local_p, # local_clustered,\n",
    "    ascending_clustered,\n",
    "    sensmns,\n",
    "    fragments\n",
    "])\n",
    "print(clustered_pMN_to_MN_df.shape)\n",
    "\n",
    "clustered_pMN_to_MN_df_no_frags = pd.concat([\n",
    "    descending_clustered,\n",
    "    sensory_clustered,\n",
    "    intrseg_clustered,\n",
    "    local_p, # local_clustered,\n",
    "    ascending_clustered,\n",
    "])\n",
    "print(clustered_pMN_to_MN_df_no_frags.shape)\n",
    "\n",
    "clustered_pMN_to_MN_df_with_clusteredvnc = pd.concat([\n",
    "    descending_clustered,\n",
    "    sensory_clustered,\n",
    "    intrseg_clustered,\n",
    "    local_clustered,\n",
    "    ascending_clustered,\n",
    "])\n",
    "print(clustered_pMN_to_MN_df_with_clusteredvnc.shape)\n",
    "\n",
    "# ordered_pMN_to_MN_df_no_desc = pd.concat([\n",
    "#     sensory,\n",
    "#     vncns,\n",
    "#     ascending\n",
    "# ])\n",
    "# print(ordered_pMN_to_MN_no_desc_df.shape)\n",
    "\n",
    "\n",
    "# ordered_pMN_to_MN_df_all = pd.concat([\n",
    "#     descending,\n",
    "#     sensory,\n",
    "#     vncns,\n",
    "#     ascending,\n",
    "#     fragments\n",
    "# ])\n",
    "# print(ordered_pMN_to_MN_df_all.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the clustered_pMN_to_MN_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(clustered_pMN_to_MN_df,ext='clustered_ordered')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Motor neuron table and determine cell classes \n",
    "My current hierarchy would be: Region (brain,vnc,descending,ascending), segment (T1,2,3,Ab), class (motor, sensory, intersegmental, local), neurotransmitter, primary neurite bundle/hemilineage, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pre_to_mn_df = clustered_pMN_to_MN_df # connectome_create.load_pre_to_mn_df(ext='clustered_ordered')\n",
    "\n",
    "all_pre_to_mn_df['cell_class']  = ''\n",
    "print(all_pre_to_mn_df.shape)\n",
    "\n",
    "All = slice(None)\n",
    "                    # segID\tmotor\thas_sma\tsensory\tneck\tlocal   T1_count\n",
    "all_pre_to_mn_df.loc[(All,  True,   True,   False,  False,  False,  All),'cell_class'] = 'motor'\n",
    "all_pre_to_mn_df.loc[(All,  True,   True,   True,  False,  False,  All),'cell_class'] = 'motor'\n",
    "all_pre_to_mn_df.loc[(All,  False,  False,    True,   False,  False,  All),'cell_class'] = 'sensory' \n",
    "all_pre_to_mn_df.loc[(All,  False,  True,    True,   False,  False,  All),'cell_class'] = 'sensemns' # always has those weird neurons in there\n",
    "all_pre_to_mn_df.loc[(All,  False,  False,  False,  True,   False,  All),'cell_class'] = 'descending'\n",
    "all_pre_to_mn_df.loc[(All,  False,  True,   False,  True,   False,  All),'cell_class'] = 'ascending'\n",
    "all_pre_to_mn_df.loc[(All,  False,  False,  False,  False,  False,  All),'cell_class'] = 'fragment'\n",
    "all_pre_to_mn_df.loc[(All,  False,  True,   False,  False,  True,   All),'cell_class'] = 'local'\n",
    "all_pre_to_mn_df.loc[(All,  False,  True,   False,  False,  False,  All),'cell_class'] = 'intersegmental'\n",
    "print(all_pre_to_mn_df.cell_class.value_counts())\n",
    "\n",
    "all_pre_to_mn_df = all_pre_to_mn_df.set_index(keys=['cell_class'],append=True,drop=True,verify_integrity=True)\n",
    "all_pre_to_mn_df = all_pre_to_mn_df.reset_index(level=['motor','has_soma','sensory','neck','local','non_T1_count'],drop=True)\n",
    "all_pre_to_mn_df = all_pre_to_mn_df.reorder_levels(['cell_class','segID'],'index')\n",
    "print(all_pre_to_mn_df.shape)\n",
    "\n",
    "pre_to_mn_df = all_pre_to_mn_df.loc[(['descending','sensory','intersegmental','local','ascending']),:]\n",
    "print(pre_to_mn_df.shape)\n",
    "\n",
    "pre_to_mn_df.head()\n",
    "\n",
    "\n",
    "# Note 8/2/22 - the the sensory group is currently 117. This includes 111 actual sensory neurons, likely after some proofreading. \n",
    "# Then there are 6 sensory neurons with somas. This should change on 8/3, there are still the bilateral mns, but some of the sensory snarls have been proofread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(all_pre_to_mn_df,ext='pre_match_to_pool_w_fragments')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning preMNs to motor modules\n",
    "This script answers the following question: Is there a stable match where we can assign a local premotor neuron to a muscle group?\n",
    "The idea comes from the med school matching algorithm and aims to produce an order to the premotor neurons where the \"most important\" neurons for a given muscle is assigned to that muscle. This will allow us to look at the connectivity of the local premotors and say, e.g., \"this important neuron for tibia flexors is also connected to femur flexors\". So, what does \"important\" mean? To assign premotors to motor modules, for each premotor neuron, we rank the motor modules by the number of combined inputs to the motor modules. For each motor modules, we rank premotor neurons by their contribution to the cluster unit vector. Then we let the matching algorithm work on those rankings. This ranking system is premotor/student optimal, and the lists for the motor modules/school are not limited, so it should aggregate premotor neurons just to their prefered module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from match.RothPeranson import MatchController\n",
    "plot_logical = False\n",
    "show_logical = False\n",
    "\n",
    "new_pre_to_mn = None\n",
    "sho_pre_to_mn = None\n",
    "\n",
    "pool_capacity = 65  # This limits how many neurons make up the main tibia flexor pool\n",
    "pool_capacity = 200 # This allows nearly as many premotor neurons into any pool that want to\n",
    "\n",
    "mpool_dict = utils.get_motor_pool_tuple_dict()\n",
    "pool_keys = [\n",
    "    'thorax_swing',\n",
    "    'thorax_stance',\n",
    "    'trochanter_extension',\n",
    "    'trochanter_flexion',\n",
    "    'femur_reductor',\n",
    "    'tibia_extensor',\n",
    "    'main_tibia_flexor',\n",
    "    # 'auxiliary_tibia_flexor_A',\n",
    "    'auxiliary_tibia_flexor_B',\n",
    "    'auxiliary_tibia_flexor_E',\n",
    "    'ltm',\n",
    "    'tarsus_depressor_med_venU',\n",
    "    'tarsus_depressor_noid',\n",
    "    ]\n",
    "\n",
    "# quick sanity check, all the mns assigned to a pool\n",
    "# total=0\n",
    "# for key in pool_keys:\n",
    "#     a = pre_to_mn_df.loc[:,mpool_dict[key]].shape[1]\n",
    "#     total = total+a\n",
    "#     print('{}: {}'.format(key,a))\n",
    "# print('{} total'.format(total))\n",
    "\n",
    "for cclass_tup in [('descending',All,), ('sensory',All,), ('ascending',All,),('intersegmental',All,),('local',All,)]:\n",
    "# for cclass_tup in [('local',All,)]:\n",
    "\n",
    "    cell_class = pre_to_mn_df.loc[cclass_tup,:]\n",
    "\n",
    "    figfname = './figpanels/{}_match_matrix.svg'.format(cclass_tup[0])\n",
    "    ylabelstring = '{} neurons'.format(cclass_tup[0])\n",
    "    print('Starting {}, {} premotor neurons'.format(ylabelstring,cell_class.shape[0]))\n",
    "\n",
    "    # Make a data frame of inputs to a pool\n",
    "    pre_to_pool_df = None\n",
    "    for key in pool_keys:\n",
    "        if pre_to_pool_df is None:\n",
    "            pre_to_pool_df = cell_class.loc[:,mpool_dict[key]].sum(axis=1)\n",
    "        else:    \n",
    "            pre_to_pool_df = pd.concat([pre_to_pool_df,cell_class.loc[:,mpool_dict[key]].sum(axis=1)],axis=1)\n",
    "    pre_to_pool_df.columns = pool_keys\n",
    "    pre_to_pool_df\n",
    "\n",
    "    # Rank the motor pools for each premotor neuron\n",
    "    pre_to_pool_ranks_df = pre_to_pool_df.assign(**pre_to_pool_df.iloc[:, 0:].rank(axis = 1, ascending = False).astype(int))\n",
    "    pre_to_pool_ranks_df.index = pre_to_pool_ranks_df.index.get_level_values('segID')\n",
    "\n",
    "    # take the transpose, so each premotor neuron is a column and fill in the values with the ranked pool\n",
    "    premn_prefs_for_pools = pre_to_pool_ranks_df.T\n",
    "    pools = np.array(premn_prefs_for_pools.index.to_list())\n",
    "    for idx in premn_prefs_for_pools:\n",
    "            premn_prefs_for_pools[idx] = pools[(premn_prefs_for_pools[idx].to_numpy(dtype=int)- 1).argsort()]\n",
    "    premn_prefs_for_pools = premn_prefs_for_pools.reset_index(drop=True)\n",
    "    premn_prefs_for_pools.columns = premn_prefs_for_pools.columns.astype(str)\n",
    "\n",
    "    # Now create a DF where the motor pools rank the premotor neurons. \n",
    "    # Do this by ranking along the pool\n",
    "\n",
    "    pool_ranks_of_pre_df = pre_to_pool_df.assign(**pre_to_pool_df.iloc[0:,:].rank(axis = 0, ascending = False).astype(int))\n",
    "    pool_ranks_of_pre_df.index = pool_ranks_of_pre_df.index.get_level_values('segID')\n",
    "\n",
    "    # rank the premotors\n",
    "    pool_prefs_for_premns = pool_ranks_of_pre_df\n",
    "    premns = np.array(pool_prefs_for_premns.index.astype(str).to_list())\n",
    "    for idx in pool_prefs_for_premns:\n",
    "            pool_prefs_for_premns[idx] = premns[(pool_prefs_for_premns[idx].to_numpy(dtype=int)- 1).argsort()]\n",
    "    pool_prefs_for_premns = pool_prefs_for_premns.reset_index(drop=True)\n",
    "\n",
    "    pool_capacities = pool_prefs_for_premns.count()-(pool_prefs_for_premns.shape[0]-pool_capacity)\n",
    "    pool_capacities_df = pool_capacities.to_frame()\n",
    "\n",
    "    pool_capacities_df = pool_capacities_df.rename(columns = {0: 'places'})\n",
    "\n",
    "    ## Now run the match algorithm\n",
    "    match = MatchController(pool_prefs_for_premns, premn_prefs_for_pools,pool_capacities_df)\n",
    "    match.start_match()\n",
    "    match_dict = match.results_dict()\n",
    "\n",
    "    # Now reorder according to the match\n",
    "    lidx = cell_class.index.to_frame()\n",
    "    lidx['preferred_pool'] = ''\n",
    "    for idx,row in lidx.iterrows():\n",
    "        lidx.loc[idx,'preferred_pool'] = match_dict[str(row.segID)]\n",
    "    lidx\n",
    "        \n",
    "    cell_class.index = pd.MultiIndex.from_frame(lidx)\n",
    "    cell_class = cell_class.reorder_levels(['cell_class','preferred_pool','segID'],'index')\n",
    "\n",
    "    s = cell_class.sum(axis=1)\n",
    "    cell_class = cell_class.loc[s.sort_values(ascending=False).index,:]\n",
    "    cell_class = cell_class.loc[(All,pool_keys,All),:]\n",
    "\n",
    "    cell_class.head()\n",
    "\n",
    "    %config InlineBackend.figure_formats = ['png']\n",
    "    %matplotlib inline\n",
    "\n",
    "    if plot_logical:\n",
    "        cmap = utils.white_dense()\n",
    "\n",
    "        mn_mi = cell_class.columns.to_frame()\n",
    "        lbls = utils.mn_labels(mn_mi,depth='rank')\n",
    "        fig = plt.figure(1, figsize = [15,25])\n",
    "        ax = sns.heatmap(np.log10(cell_class.to_numpy()+1), xticklabels=lbls, cmap=cmap)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        # here set the labelsize by 20\n",
    "        # cbar.ax.tick_params(labelsize=20)\n",
    "        cbar.set_label(label = 'log 10 # of synapses', size=24)\n",
    "        plt.xlabel('Motor Neurons', fontsize =18)\n",
    "        plt.ylabel(ylabelstring, fontsize =18)\n",
    "        plt.yticks(fontsize = 16)\n",
    "        plt.xticks(fontsize = 16)\n",
    "        plt.show()\n",
    "\n",
    "        fig.savefig(figfname,format='svg')\n",
    "\n",
    "    if show_logical:\n",
    "        # put in a gap, just for show\n",
    "        cell_class_for_show = None\n",
    "        pool_gap = cell_class.iloc[:,0].copy().to_frame()\n",
    "        for col in pool_gap.columns:\n",
    "            pool_gap[col].values[:] = 2\n",
    "\n",
    "        cell_class_for_show = None\n",
    "        for key in pool_keys:\n",
    "            if cell_class_for_show is None:\n",
    "                cell_class_for_show = cell_class.loc[:,mpool_dict[key]].copy()\n",
    "            else:    \n",
    "                cell_class_for_show = pd.concat([cell_class_for_show,pool_gap.copy()],axis=1)\n",
    "                cell_class_for_show = pd.concat([cell_class_for_show,cell_class.loc[:,mpool_dict[key]]],axis=1)\n",
    "    \n",
    "        class_gap = cell_class_for_show.iloc[0:10,:].copy()\n",
    "        for col in class_gap.columns:\n",
    "            class_gap[col].values[:] = 2\n",
    "        if sho_pre_to_mn is None:\n",
    "            sho_pre_to_mn = cell_class_for_show.copy()\n",
    "        else:\n",
    "            sho_pre_to_mn = pd.concat([sho_pre_to_mn,class_gap.copy()],axis=0)\n",
    "            sho_pre_to_mn = pd.concat([sho_pre_to_mn,cell_class_for_show],axis=0)\n",
    "\n",
    "    if new_pre_to_mn is None:\n",
    "        new_pre_to_mn = cell_class\n",
    "    else:\n",
    "        new_pre_to_mn = pd.concat([new_pre_to_mn,cell_class],axis=0)\n",
    "\n",
    "\n",
    "if show_logical:\n",
    "    fig = plt.figure(1, figsize = [15,25])\n",
    "    ax = sns.heatmap(np.log10(sho_pre_to_mn.to_numpy()+1), xticklabels=lbls, cmap=cmap)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    # here set the labelsize by 20\n",
    "    # cbar.ax.tick_params(labelsize=20)\n",
    "    cbar.set_label(label = 'log 10 # of synapses', size=24)\n",
    "    plt.xlabel('Motor Neurons', fontsize =18)\n",
    "    plt.ylabel(ylabelstring, fontsize =18)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.xticks(fontsize = 16)\n",
    "    plt.show()\n",
    "\n",
    "    fig.savefig('matched_premotor_to_motor',format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(new_pre_to_mn,ext='matched_to_pool')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add local neuron classification and hemilineage to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_to_mn_df = new_pre_to_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmn_annotation_table = client.materialize.query_table('t1l_local_premotor_table_v3',timestamp=connectome_create.get_timestamp())\n",
    "\n",
    "\n",
    "# pmn_annotation_table.classification_system.value_counts()\n",
    "# pmn_annotation_table.cell_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.save_df_csv(pmn_annotation_table,name='t1l_local_premotor_table_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the pmn seg is are not in the matrix\n",
    "pmn_index = pre_to_mn_df.index.to_frame()\n",
    "pmn_index = pmn_index.set_index(keys=['segID'],drop=False)\n",
    "local_pmns  = pmn_index.loc[pmn_index.cell_class=='local']\n",
    "\n",
    "\n",
    "local_pmns_to_type = local_pmns.loc[~local_pmns.segID.isin(pmn_annotation_table.pt_root_id)]\n",
    "utils.save_df_csv(local_pmns_to_type,name='t1l_local_premns_to_type')\n",
    "local_pmns_to_type.segID.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just add classifications and hemilineage to the index, then put the index back on and reorder\n",
    "pmn_index = pre_to_mn_df.index.to_frame()\n",
    "pmn_index = pmn_index.set_index(keys=['segID'],drop=False)\n",
    "\n",
    "pmn_index['classification_system'] = None\n",
    "pmn_index['cell_type'] = None\n",
    "\n",
    "for clsys in pmn_annotation_table.classification_system.unique():\n",
    "    pmn_index.loc[pmn_index.segID.isin(pmn_annotation_table.loc[pmn_annotation_table.classification_system==clsys,'pt_root_id']),'classification_system'] = clsys\n",
    "    \n",
    "for ctype in pmn_annotation_table.cell_type.unique():\n",
    "    pmn_index.loc[pmn_index.segID.isin(pmn_annotation_table.loc[pmn_annotation_table.cell_type==ctype,'pt_root_id']),'cell_type'] = ctype\n",
    "\n",
    "pre_to_mn_df.index = pd.MultiIndex.from_frame(pmn_index)\n",
    "pre_to_mn_df = pre_to_mn_df.reorder_levels(['cell_class','preferred_pool','classification_system','cell_type','segID'],'index')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add neurotransmitters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = pre_to_mn_df.index.get_level_values('cell_type').unique().to_numpy()\n",
    "nt_table = pd.read_csv('./annotations_hl/LacinHLTable.csv')\n",
    "import re\n",
    "pattern = re.compile(\"R[0-9]\")\n",
    "\n",
    "hl_lut = {'cell_type':cell_types,'hemilineage':cell_types}\n",
    "hl_lut = pd.DataFrame(data=hl_lut)\n",
    "\n",
    "for idx,r in hl_lut.iterrows():\n",
    "    try:\n",
    "        if np.isnan(r.hemilineage):\n",
    "            continue\n",
    "    except TypeError:\n",
    "        pass\n",
    "    hl = r.hemilineage\n",
    "\n",
    "    if hl[0:2]=='RD':\n",
    "        hl = '24B'\n",
    "        # print('string was RD, now {}'.format(hl))\n",
    "\n",
    "    if pattern.match(hl) or hl=='RVD' or hl=='Rcore_':\n",
    "        # print('string was {}, now 8A'.format(hl))\n",
    "        hl = '8A'\n",
    "\n",
    "    if not hl.isalnum():\n",
    "        hl = hl[0:-1]\n",
    "        # print(hl)\n",
    "    \n",
    "    if not hl.isalnum():\n",
    "        hl=hl[0:hl.find('_')]\n",
    "        # print(hl)\n",
    "\n",
    "    if hl == '9Ac':\n",
    "        hl='9A'\n",
    "        # print(hl)\n",
    "\n",
    "    hl_lut.loc[idx,'hemilineage'] = hl\n",
    "\n",
    "# merge with neurotransmitter table to get a lookuptable\n",
    "hl_lut = hl_lut.merge(nt_table,how='outer',left_on='hemilineage',right_on='HL')\n",
    "hl_lut = hl_lut.loc[~hl_lut.hemilineage.isna(),:]\n",
    "\n",
    "hl_lut_reduced = hl_lut[['cell_type','NT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just operate on the index, where the nt will be\n",
    "pmn_index_df = pre_to_mn_df.index.to_frame().reset_index(drop=True)\n",
    "\n",
    "# join the index on cell_type, with NT\n",
    "pmn_index_df = pmn_index_df.join(hl_lut_reduced.set_index('cell_type'),how='left',on='cell_type')\n",
    "\n",
    "# reorder the matrix\n",
    "pmn_index_df = pmn_index_df[['cell_class','preferred_pool','NT','classification_system','cell_type','segID']]\n",
    "pmn_index_df.NT.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify sensory neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sensory = connectome_create.get_unduplicated_sensory_axon_table(client)\n",
    "\n",
    "all_sensory = all_sensory[['pt_root_id','classification_system','cell_type']]\n",
    "all_sensory = all_sensory.rename({'pt_root_id':'segID'},axis=1)\n",
    "\n",
    "# join the index on cell_type, with NT\n",
    "pmn_index_df = pmn_index_df.merge(all_sensory,how='left',on='segID',suffixes=['_x','_y'])\n",
    "\n",
    "sens_rows = (pmn_index_df.cell_class == 'sensory') & (pmn_index_df.classification_system_x.isna())\n",
    "pmn_index_df.loc[sens_rows,'classification_system_x'] = pmn_index_df.loc[sens_rows].classification_system_y\n",
    "pmn_index_df.loc[sens_rows,'cell_type_x'] = pmn_index_df.loc[sens_rows].cell_type_y\n",
    "\n",
    "pmn_index_df.loc[pmn_index_df.cell_class == 'sensory','NT'] = 'Ach'\n",
    "pmn_index_df = pmn_index_df.drop(['classification_system_y','cell_type_y'],axis=1)\n",
    "pmn_index_df = pmn_index_df.rename(columns={'classification_system_x':'classification_system','cell_type_x':'cell_type'})\n",
    "\n",
    "pre_to_mn_df.index = pd.MultiIndex.from_frame(pmn_index_df)\n",
    "\n",
    "glia = pre_to_mn_df[pre_to_mn_df.index.get_level_values('cell_type') == 'Gial_error']\n",
    "no_glia = pre_to_mn_df[pre_to_mn_df.index.get_level_values('cell_type') != 'Gial_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectome_create.save_pre_to_mn_df(no_glia,ext='matched_typed_with_nt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_glia.index.get_level_values('cell_class').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_glia.index.get_level_values('preferred_pool').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_glia.index.get_level_values('NT').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_glia.index.get_level_values('classification_system').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_glia.index.get_level_values('cell_type').value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_glia.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b93c3ce0f0b01938714f8d6ce3882059af39419bb08f14e121c5729b1321faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
